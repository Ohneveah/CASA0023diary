[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA 0023 Learning Diary",
    "section": "",
    "text": "Personal Introduction\nHi, I’m Xingru Liu. I majored in Financial Mathematics at Xijiao Liverpool University. During my 4 years of undergraduate study, I learned a lot of mathematical modeling and quantitative analysis methods related to financial markets, and also developed my interest and ability in programming and analysis. Suzhou is famous for its unique gardens, and the delicacy and beauty of these gardens have sparked my interest in urban spatial science and inspired me to explore how to combine the design principles of gardens with the analytical skills I have learned in financial math to create aesthetically pleasing and functional urban spatial solutions in my future career. I believe that new possibilities in urban planning and development can be found through this interdisciplinary fusion."
  },
  {
    "objectID": "Week 1 - Introduction to Remote Sensing.html#summary",
    "href": "Week 1 - Introduction to Remote Sensing.html#summary",
    "title": "Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThe summary of Lecture 1 will I show in a hand-drawn mind map as shown below, as I referred to my study journals from previous years and felt that this format was easier to understand, and also allowed me to memorize as I took notes."
  },
  {
    "objectID": "Week 1 - Introduction to Remote Sensing.html#application",
    "href": "Week 1 - Introduction to Remote Sensing.html#application",
    "title": "Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote sensing technology has shown great potential for decision support in agricultural production management, which involves from pre-field preparation, seasonal crop health monitoring, to the final harvesting process. For example, in Preseason Planning, digital elevation models (DEMs) depicted by remote sensing imagery are needed to derive field topography and help agricultural decision makers to analyze land conditions in order to decide on the most suitable crop types and planting densities for spring planting (Khanal et al., 2020). Second, remote sensing synergizes with other advanced technologies, and the figure below shows how remote sensing and the Internet of Things (IoT) with smart sensors can be combined and applied specifically in agriculture.\n\n(Ullo and Sinha, 2021)\nRemote sensing technologies also play a crucial role in military domains such as surveillance and reconnaissance, positioning, navigation and timing (PNT), communications, missile warning, and satellite command and control (C2) architectures (Veterans Today, 2021). Of even greater interest is the fact that remote sensing data and technologies are widely used in urban studies to analyze various forms of interaction of social, ecological, and technological systems in different geographic contexts (Wellmann et al., 2020). It is evident that remote sensing data can be an important tool to provide environmental analysis independently and in a cost-efficient manner, thus actively contributing to policy making, public participation and detailed planning strategies."
  },
  {
    "objectID": "Week 1 - Introduction to Remote Sensing.html#reflection",
    "href": "Week 1 - Introduction to Remote Sensing.html#reflection",
    "title": "Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nBefore I took this course, “remote sensing” gave me the feeling that it was very advanced, and it would be very strange and abstract, requiring me to spend a lot of time to understand and explore it. However, from this class, I realized that remote sensing is used everywhere in our daily life. For example, when I was a child, I saw the weather forecast from inside the TV. Much of the data in it comes from weather satellites. They pass through a variety of sensors, thus capturing data on various aspects of the Earth, including the movement of clouds, changes in temperature and humidity, precipitation, wind speed and direction, and so on. Secondly, I use Google Maps for my daily travels, where the Global Positioning System (GPS) and online mapping services use remote sensing data to provide geographic information and real-time traffic conditions, telling me how I should get to my destination and how long the journey will take. What I didn’t realize was that I had previously done a study on atmospheric pollutants that affect PM2.5, and the data for that was also obtained through remote sensing. I realized that “remote sensing” is not as strange as I thought, and I want to learn more about it."
  },
  {
    "objectID": "Week 1 - Introduction to Remote Sensing.html#references",
    "href": "Week 1 - Introduction to Remote Sensing.html#references",
    "title": "Week 1 - Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\nKhanal, S., KC, K., Fulton, J. P., Shearer, S. and Ozkan, E. (2020) ‘Remote sensing in agriculture-accomplishments, limtations and opportunities’, Remote Sensing and IoT for Smart Learning Environments, 12(22), pp. 3783.\nUllo, S. L. and Sinha, G. R. (2021) ‘Advances in IoT and smart sensors for remote sensing and agriculture applications’, Internet of Remote Things for Remote Sensing,13(13), pp. 2585.\nVeterans Today (2021) Available at: https://veteranstoday.com/2021/04/24/military-space-satellites/ (Accessed: 24 January 2024).\nWellmann, T., Lausch, A., Andersson, E., Knapp, S., Cortinovis, C., Jache, J., Scheuer, S., Kremer, P., Mascarenhas, A., Kraemer, R., Haase, A., Schug, F. and Haase, D. (2020) ‘Remote sensing in urban planning: Contributions towards ecologically sound policies?’, Landscape and Urban Planning, 204, pp. 103921."
  },
  {
    "objectID": "Week 2 - Xaringan.html",
    "href": "Week 2 - Xaringan.html",
    "title": "Week 2 - Xaringan",
    "section": "",
    "text": "Week 2 consisted of a presentation on the Aeolus satellite at Xaringan. The presentation is shown below and the summary, applications and reflections are included in the slides."
  },
  {
    "objectID": "Week 3 - Corrections.html#summary",
    "href": "Week 3 - Corrections.html#summary",
    "title": "Week 3 - Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary"
  },
  {
    "objectID": "Week 3 - Corrections.html#application",
    "href": "Week 3 - Corrections.html#application",
    "title": "Week 3 - Corrections",
    "section": "3.2 Application",
    "text": "3.2 Application\nThe practice sessions covered remote sensing data processing, atmospheric correction, acquiring and merging images, and enhancements such as ratio analysis, filtering, texture analysis, data fusion, and principal component analysis (PCA). This application section will focus on applications related to the use of atmospheric correction. Atmospheric correction techniques play an important role in the field of environmental monitoring, especially in the quality assessment of marine and inland waters. For example, wang et al. (2024) proposed an atmospheric correction algorithm designed for optically complex water bodies at high solar zenith angles, which utilizes the Extreme Gradient Boosting (XGBoost) technique, which can improve the accuracy and coverage of mapping of suspended solids in the marine environment for monitoring marine and coastal ecosystems affected by human activities and natural factors. In addition, the Atmospheric Correction Algorithm for Inland Waters (ACbTC) utilizes synergistic observations from the Ocean and Land Color Instrument (OLCI) and the Sea and Land Surface Temperature Radiometer (SLSTR) to improve the accuracy of color information in inland waters (Bi et al., 2018). This approach is particularly important for monitoring inland lakes that are increasingly impacted by human pressures and climate change.\n\n\n\n\n\nComparison of atmospheric corrections for MODIS scenes in the eastern Mediterranean: SMAC vs. PySpectral algorithms (Scheirer et al., 2018)\nAtmospheric calibration still has some problems. On the one hand, existing algorithms such as SMAC and CREFL may not be applicable to all sensors, and radiative transfer calculations are time-consuming and costly for real-time operation (Scheirer et al., 2018). In future research, there is a need to continue to improve the calibration consistency between sensors to be improved, and to simplify the algorithms to enhance the feasibility of real-time applications. On the other hand, the existing forecast models are not able to obtain the actual atmospheric aerosol loads accurately enough, affecting the accuracy of satellite retrievals (Scheirer et al., 2018)."
  },
  {
    "objectID": "Week 3 - Corrections.html#reflection",
    "href": "Week 3 - Corrections.html#reflection",
    "title": "Week 3 - Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nI must admit that this lecture was difficult for me to understand! Remote sensing imagery is much more complex than I thought it would be, but it was interesting to learn more about how imagery can be processed to show specific things, and how it can be improved to maximize its accuracy and potential. It is clear, however, that there are many ways to enhance, correct, or tweak an image, so it is important to keep specific goals in mind when making these changes. From the lectures and the hands-on content, I found DOS calibration to be particularly interesting - I think it’s an ingenious and straightforward approach to atmospheric calibration.\nI think it’s useful to understand the theory behind the corrections, but I’m not sure it’s important to understand the specifics and scientific rationale behind each type, since so much of the data is corrected. Nonetheless, I think it is important in data science and research to not analyze the data as it is, but to think about how the data was created or adjusted. In the future, I will be thinking more about this, whether I am working with remotely sensed data or other types of data.\nThroughout this week, remote sensing imagery may contain some errors, but how are these errors corrected and what is the appropriate time to correct them? It is confusing to me. And, how to choose the method of correction and is the remotely sensed data correct after correction? This question put me in deep thought. I feel that if I want to understand and skillfully master various correction methods, I need to read more related books and research, and have a certain amount of knowledge reserve to better understand the data correction.\nSecondly, about the connection and enhancement of data. I think this is very interesting. Before, when studying casa0001 to capture the changes of urban development through pictures, if enhancement of satellite images can help us capture some more detailed points. It can also help us to track the development of some things. But it feels that it is still difficult to enhance this technology and innovate the dataset. And, some times, we can’t analyze it well based on the images.\nI hope that in the subsequent research, we can understand the data filtering and processing methods, as well as and develop our own ability to analyze the pictures."
  },
  {
    "objectID": "Week 3 - Corrections.html#references",
    "href": "Week 3 - Corrections.html#references",
    "title": "Week 3 - Corrections",
    "section": "3.4 References",
    "text": "3.4 References\nBi, S., Li, Y. M., Wang, Q., Lyu, H., Liu, G., Zheng, Z. B., Du, C. G., Mu, M., Xu, J., Lei, S. H. and Miao, S. (2018) ‘Inland Water Atmospheric Correction Based on Turbidity Classification Using OLCI and SLSTR Synergistic Observation’, Remote Sensing, 16(1), pp. 183.\nScheirer, R., Dybbroe, A. and Raspaud, M. (2018) ‘A General Approach to Enhance Short Wave Satellite Imagery by Removing Background Atmospheric Effects’, Remote Sensing, 10(4), pp. 560.\nWang, Y. Q., Liu, H. Z., Zhang, Z. X., Wang, Y. R., Li, Q. Q. and Wu, G. F. (2024) ‘Ocean Colour Atmospheric Correction for Optically Complex Waters under High Solar Zenith Angles: Facilitating Frequent Diurnal Monitoring and Management’, Remote Sensing, 16(1), pp. 183."
  },
  {
    "objectID": "Week 4 - Policy.html#summary",
    "href": "Week 4 - Policy.html#summary",
    "title": "Week 4 - London Flooding",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nLondon, the economic heart of the United Kingdom and a major hub for international commerce, has a vast borough covering 1,573 square miles of Greater London. As the population continues to grow, urban development is gradually expanding into the Thames Estuary. Areas along the Thames, particularly the City of Finance and Canary Wharf, are densely populated with commercial activity, but these areas are also at significant risk of flooding, with more than 350 square miles of land (approximately 22.25%) located in flood-prone floodplains.\n\n(Source: Yunus, et al., 2016)\nIn July 2021, London was hit by severe flooding, a disaster triggered by extreme rainfall that flooded roads and tube stations, disrupted transportation and forced residents to evacuate. The flooding had a profound impact on people’s daily lives, turning streets into waterways, trapping vehicles and causing hardship for residents as they fled. The City of London and relevant departments are actively assessing and improving flood protection measures, developing strategies to respond to extreme weather events and mitigating the effects of climate change. Emergency services, such as the Fire Service, responded quickly to carry out rescue missions during the floods to ensure the safety of citizens.\nThe London Roundtable’s progress report noted that the flooding caused severe damage to homes and infrastructure in the city, resulting in many residents needing to be relocated, damage to critical infrastructure, the closure or partial closure of 30 Tube stations, and the forced evacuation of hospital wards and schools. Some schools have not been able to resume classes so far. The floods have also affected parts of London, disrupting road and rail traffic, flooding stores and offices, severely affecting Tube lines and overground rail services, and making major commuter routes such as the A4 and M23 impassable, affecting commuters.\nIt is predicted that without flood management measures, annual expected losses in London will rise from the current £2 billion to between £2.7 and £3 billion as the climate warms, with the risk rising further to between £3.5 and £3.9 billion if temperatures rise by 4°C. The UK government publishes a Climate Change Risk Assessment (CCRA) every five years under the Climate Change Act 2008 and has implemented a £15 million Natural Flood Management (NFM) program between 2017 and 2021 to explore effective flood management techniques to meet future challenges.\nThe UK Environment Agency (EA) this week unveiled a multi-billion pound plan to protect London from flooding, but the regulator said there is a ‘funding gap’ that needs to be filled (McGlone, 2024).\nLondon has implemented a comprehensive range of policies and measures to address flooding challenges that reflect the city’s commitment to resilience and sustainability. Below are a few of London’s key strategies for flood protection:\n\nLondon Resilience Strategy: In February 2020, London published this strategy, which aims to increase the city’s resilience to a range of shocks and stresses, including flood risk. The strategy emphasizes the importance of cross-sectoral collaboration and community engagement to ensure effective risk management and response.\nThe Thames Flood Barrier: as a key component of London’s flood protection system, the Thames Flood Barrier is able to close at times of unusually high tides, providing London with protection from the damage caused by tidal flooding.\nThames Basin Management Plan: This long-term plan aims to manage water resources and flood risk in the Thames Basin through a combination of measures. Plans include improving the riverbanks, upgrading the capacity of the drainage system.\nSustainable Urban Drainage Systems (SuDS): London actively promotes the use of SuDS, such as permeable paving, rain gardens and green roofs, which help to reduce the risk of flooding by reducing surface runoff and increasing the natural infiltration and retention of rainwater.\nFlood warning and information services: The UK Environment Agency provides key flood warning services to enable residents and businesses to receive timely information on potential flooding and to prepare for it. The London government and related departments also provide flood-related information and response guidelines through a variety of channels to raise public awareness of preparedness and response capabilities.\n\nThis issue and its solutions are also directly related to the SDG targets:\n\n11.3 - Inclusive urbanization and sustainability of cities, where the tiered targets reduce the adverse effects of natural disasters and implement policies for inclusion(Our World in Data team, 2023).\n13.1 - Resilience and adaptation to climate-related and natural disasters (Our World in Data team, 2023)."
  },
  {
    "objectID": "Week 4 - Policy.html#application",
    "href": "Week 4 - Policy.html#application",
    "title": "Week 4 - London Flooding",
    "section": "4.2 Application",
    "text": "4.2 Application\nThe flood problem in London is a complex environmental challenge in which the application of remote sensing technology provides an important tool for monitoring, assessing and responding to floods. The following are some of the key applications of combining remote sensing data to the flood problem in London:\n\nFlood monitoring and assessment:\nUsing Synthetic Aperture Radar (SAR) data(Cruz, et al., 2022), it is possible to penetrate cloud cover and obtain real-time images of the extent of flooding and affected areas. This is critical for rapid response and assessment of damage caused by flooding.\nMulti-temporal remote sensing data allows monitoring of the dynamics of flooding, such as the rate and extent of spreading of floodwaters and the recovery of floodwaters after they have receded.\nLand cover change analysis:\nUsing high-resolution optical remote sensing data, such as Sentinel-2 data(Phiri, et al., 2020), it is possible to monitor land cover changes during urbanization, including the impact of urban sprawl on flood-prone areas.\nAnalyzing historical and current land cover data, high flood risk areas can be identified and provide a scientific basis for urban planning and land management.\nInfrastructure and critical asset protection:\nRemote sensing data can be used to identify and map critical infrastructure, such as transportation networks, hospitals, schools, etc., information that is essential for flood emergency management and post-disaster recovery.\nCombined with Geographic Information Systems (GIS) and remote sensing data, the potential threat of flooding to these critical assets can be assessed and protective measures can be developed accordingly.\nFlood early warning systems:\nThe accuracy and timeliness of flood warning systems can be improved by integrating real-time remotely sensed data (precipitation data: M3HP, M10P and DG25, river data: DTR and RD) and advanced hydrologic modeling.\nRemote sensing data can also be used to monitor river levels and flows, providing key input parameters for flood warning.\n\n(Source: Yunus, et al., 2016)\nClimate change adaptation planning:\nLong-term series of remote sensing data can help analyze the impacts of climate change on flooding patterns and support the development of adaptive planning.\nMonitoring the urban heat island effect and changes in green space cover, remote sensing data can guide urban planning to enhance the climate resilience of cities.\n\nIn addressing this challenge, urban planners need to think carefully about the constraints and what they must be aware of.\n\nFinancial constraints\nSkills required\nDifferent objectives, priorities and regulatory environments. For example, data is not always shared due to commercial and legal sensitivities, limiting effective collaboration.\nPublic participation and public expectations"
  },
  {
    "objectID": "Week 4 - Policy.html#reflection",
    "href": "Week 4 - Policy.html#reflection",
    "title": "Week 4 - London Flooding",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nAfter an in-depth study of the flooding problem in London, I have come to realize that it is a topic that involves a multitude of factors. London’s urban growth and change is the result of a combination of factors, which requires decision makers to consider the correlation between these factors in a comprehensive manner when formulating relevant policies. Particularly in the context of climate change, the increase in extreme weather events makes flood risk assessment and identifying vulnerable areas in cities important.\nFlooding in London is of great interest to me, and as the impacts of climate change increase, flood risk assessment and the identification of vulnerable areas will play an increasingly important role in urban planning and sustainable development. With limited resources and funding, accurately identifying the most prioritized areas is essential to effectively respond to the threat of flooding.\nLondon’s continued population growth is challenging the city’s infrastructure and public services, particularly in terms of flood risk management. I am curious about how London is responding to this growth and whether it will take steps to direct population distribution or improve development and infrastructure within the city to accommodate this growth.\nI am also interested in how London is using remote sensing data and other advanced technologies to monitor and predict flood risk. The application of these technologies helps us to better understand flooding patterns and trends, and provides policymakers with a scientific basis to more effectively develop and implement flood prevention measures. This technology-driven approach may become the key to future urban flood management."
  },
  {
    "objectID": "Week 4 - Policy.html#references",
    "href": "Week 4 - Policy.html#references",
    "title": "Week 4 - London Flooding",
    "section": "4.4 References",
    "text": "4.4 References\nCruz, H., Vestias, M., Monteiro, J., Neto, H. and Duarte, R. P. (2022) ‘A Review of Synthetic-Aperture Radar Image Formation Algorithms and Implementations: A Computational Perspective’, Remote Sensing Image Processing, 14(5), 1258.\nMcglone, C. (2023) ‘Funding gap’ in crucial plan to protect London from flooding. Available at: https://eandt.theiet.org/2023/05/19/funding-gap-crucial-plan-protect-london-flooding(Accessed: 12 February 2024).\nOur World in Data team (2023) Make cities inclusive, safe, resilient and sustainable. Available at: https://ourworldindata.org/sdgs/sustainable-cities#sdg-indicator-11-5-1-deaths-and-injuries-from-natural-disasters (Accessed: 12 February 2024).\nOur World in Data team (2023) Make cities inclusive, safe, resilient and sustainable. Available at: https://ourworldindata.org/sdgs/climate-action#target-13-2-integrate-climate-change-measures-into-policy-and-planning  (Accessed: 12 February 2024).\nPhiri, D., Simwanda, M., Salekin, S., Nyirenda, V. R., Murayama, Y. and Ranagalage, M. (2020) ‘Sentinel-2 Data for Land Cover/Use Mapping: A Review’, Urban Remote Sensing, 12(14), 2291.\nSustainable drainage (2024) Available at: https://www.susdrain.org/delivering-suds/using-suds/background/sustainable-drainage.html(Accessed: 12 February 2024).\nYunus, A. P., Avtar, R., Kraines, S., Yamamuro, M., Lindberg,  F. and Grimmond, C. S. B. (2016) ‘Uncertainties in tidally  adjusted estimates of sea level rise flooding (bathtub model)  for the Greater London’， Remote Sensing, 8 (5). 366."
  },
  {
    "objectID": "Week 5 - Google Earth Engine.html#summary",
    "href": "Week 5 - Google Earth Engine.html#summary",
    "title": "Week 5 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\n\nDefinition of Google Earth Engine\n\nGoogle Earth Engine (GEE) is a cloud-based platform specialized in Earth-scale geospatial analysis.\nStore massive datasets on servers, at very fast speeds, by writing your own code and applying it.\nGEE’s basic datatypes amount to a secondary encapsulation of JavaScript datatypes by Google, with some custom methods added for ease of use.\nCode that runs both client-side and server-side.\n\n\n\nImportance of Google Earth Engine\n\nGEE leverages Google’s cloud computing resources to provide fast data processing capabilities, making it an ideal platform for performing complex geo-spatial analysis and image processing tasks and supporting the operation of complex algorithms and models.\nThere is no need to load the full image multiple times during the loop.\nIt is free, which makes it easy for academic research, education, and non-profit programs to use the platform.\n\n\n\nSome details about Google Earth Engine\n\nGoogle Earth Engine Code Editor interface (Source: Google Earth Engine)\n\nImage scale = pixel resolution, which is actually determined by the output, not the input.\nAll displayed data are converted to Mercator projection (EPSG: 3857).\nThe object types: vectors, rasters, elements, strings and numbers, all belong to a specific category. Each category has a GEE function or method that corresponds to it.\nRaster data = lots of images\nFeature = geometry with attributes\nFeature collection = several features with attributes\nCollection = several images or polygons\nGeometry = point/line/polygon with no attributes\n\n\n\nGoogle Earth Engine functions and tools\n\nLoading image collections\nImporting, organizing, filtering and processing image data\nReducing images\nReducing images by region\nReducing images by neighbourhood\nRegression.\nLinear regression\nMultivariate multiple linear regression\nJoins and filtering\nJoin image collections\nJoin feature collections\nSpatial join\nIntersect\nSpatially subset"
  },
  {
    "objectID": "Week 5 - Google Earth Engine.html#application",
    "href": "Week 5 - Google Earth Engine.html#application",
    "title": "Week 5 - Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nThere are many keywords in Google Earth Engine (GEE), but the most revolve around Landsat Satellite, Remote Sensing, Water, Regions (Yu and Zhao, 2021). Currently, Google Earth Engine is being used in research for mining, storing, retrieving, and processing spatial data for a variety of applications designed for a wide range of areas, such as vegetation monitoring, farmland mapping, ecosystem assessment, etc (Kumar and Mutanga, 2019).\n\n\n\n\n\nKeywords related to GEE（Yu and Zhao, 2021）\nWhen I looked up the relevant literature of Google Earth Engine, many of the literature obtained Landsat images or Sentinel images from the Google Earth Engine (GEE) database and used some machine learning methods to study the issues related to land use and land cover (LULC). Ganjirad and Bagheri (2024) made and updated land use and land cover maps by analyzing Landsat 8 image data on the cloud computing platform of Google Earth Engine. In this way, environmental natural resources and land changes can be monitored in real time, and problems such as deforestation, water pollution, and urban expansion can be detected in time, so that rapid response and protection measures can be taken.\n\n\n\n\n\nExemplary image patches collected over the study region from different LULC classes. A) The high-resolution patches derived from Google Earth Pro. B) The corresponding Landsat 8 image patches. C, D) The corresponding raster representations of NDVI and BSI indices(Ganjirad and Bagheri, 2024).\nSecond, Feng et al. (2022) utilized the GEE platform to classify ecosystem types in the Yellow River Basin region of the Upper Tibetan Plateau with high accuracy by combining Sentinel-2 satellite images and machine-learning Random Forest Classification method. This study provides a rich resource for humans to understand the changes of ecosystems in the Yellow River Basin region of the upper Tibetan Plateau."
  },
  {
    "objectID": "Week 5 - Google Earth Engine.html#reflection",
    "href": "Week 5 - Google Earth Engine.html#reflection",
    "title": "Week 5 - Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nI had a lot of fun operating the Google Earth engine in my practical class this week. I have never been exposed to it before, so trying something new makes me feel very happy. I found the analytics tool to be really powerful, being able to store such a large amount of data. It’s mostly based on a JavaScript API, which can be a bit difficult for me, and it’s going to take me some time to learn this programming language. I used it to look at geographic images of some of the countries I was interested in by writing scripts. But it was a bit difficult for me to retrieve, filter and process the data efficiently. I also realized that it was very different from the GIS software I had learned before, in that the data was processed on a server rather than on a local machine. How it is processed is what I want to find out next. In the future, I would also like to understand some advanced features in GEE and learn how to write efficiently, it may help me a lot in my future participation in some projects! And it’s still free, which is a reason to want to keep learning about it."
  },
  {
    "objectID": "Week 5 - Google Earth Engine.html#references",
    "href": "Week 5 - Google Earth Engine.html#references",
    "title": "Week 5 - Google Earth Engine",
    "section": "5.4 References",
    "text": "5.4 References\nFeng, S. Y., Li, W. L., Xu, J., Liang, T. G., Ma, X. L., Wang, W. Y. and Yu, H. Y. (2022) ‘Land Use/Land Cover Mapping Based on GEE for the Monitoring of Changes in Ecosystem Types in the Upper Yellow River Basin over the Tibetan Plateau’, Remote Sensing, 14(21), 5361.\nGanjirad, M. and Bagheri, H. (2024) ‘Google Earth Engine-based mapping of land use and land cover for weather forecast models using Landsat 8 imagery’, Ecological Informatics, 80, 102498.\nKumar, L. and Mutanga, O. (2019) ‘Google Earth Engine Applications’, Remote Sensing.\nYu, L. and Zhao, Q. (2021) ‘Application of Google Earth Engine’, Available at: https://encyclopedia.pub/entry/14783 (Accessed: 27 February 2024)."
  },
  {
    "objectID": "Week 6 - Classification I.html#summary",
    "href": "Week 6 - Classification I.html#summary",
    "title": "Week 6 - Classification I",
    "section": "6.1 Summary",
    "text": "6.1 Summary\n\nHow do you do that given some imagery?\n\nInductive learning - using experience to make judgments in a given context.\nExpert systems - using human knowledge to solve problems that require human intelligence.\nKnowledge Base - rules of thumb, not always correct.\nInference Engine - process of reaching a conclusion and the expert system is implemented.\n\nMachine Learning - the use of statistical techniques to enable computers to “learn” data and then use that data to make predictions or decisions.\n\n\nClassification and Regression Trees (CART).\n\nCategorical trees - data are two and more discrete dependent variables.\nRegression tree - continuous dependent variable (data divided into very small chunks).\n\n\n(Source: CART (Classification And Regression Tree) in Machine Learning, 2023)\n\nIn the decision tree, nodes are split into sub-nodes based on a threshold value of an attribute. The root node is taken as the training set and is split into two by considering the best attribute and threshold value. Further, the subsets are also split using the same logic. This continues till the last pure sub-set is found in the tree or the maximum number of leaves possible in that growing tree.\nTree structure: CART builds a tree-like structure consisting of nodes and branches. The nodes represent different decision points, and the branches represent the possible outcomes of those decisions.\nSplitting criteria: Selects the one that best reduces the impurity of the resulting subsets. For classification tasks, CART uses Gini impurity as the splitting criterion. The lower the Gini impurity, the more pure the subset is. For regression tasks, CART uses residual reduction as the splitting criterion. The lower the residual reduction, the better the fit of the model to the data.\nPruning: To prevent overfitting of the data. Cost complexity pruning involves calculating the cost of each node and removing nodes that have a negative cost.\n\n\n\nRandom Forest - Construct multiple decision trees and summarize their predictions\n\n\n\n\n\n(Source: Tran, et al., 2023)\n\nOut-of-Bag (OOB) refers to the data samples that were not selected by bootstrap sampling in the construction of each decision tree.\nRandom Forest Algorithm Steps\n\nBootstrap sampling - repeated samples from the original data.\nWhen constructing each split node of each tree, the algorithm randomly selects a subset of features from the full set of features. The size of the selected subset of features is usually fixed.\nRandomization of node segmentation: at each node of the decision tree, a subset of features is again randomly selected.\nRepeat! We’ll get a forest of trees.\n\n\n\n\nImage Classification\n\nSupervised - in supervised learning, an algorithm learns using a set of training data that has been labeled.\nMaximumlikelihood classification - prior knowledge of the category (i.e., prior probability of the category).\nUnsupervised - unsupervised learning does not use labeled training data. Instead, it attempts to learn patterns directly from the data itself and is commonly used for tasks such as clustering or dimensionality reduction.\nK-mean clustering - discovers patterns or clusters in the data by dividing the images into K categories.\nISODATA - which adaptively adjusts the number of clusters based on the actual distribution of the data.\n\n\n\nSupport Vector Machine (SVM)\n\nLinear binary classifier, used to classify data points into two categories.\nMaximum Margin - the boundary that provides the maximum interval between the two categories.\nSupport Vector - Points on (and within) the boundary.\nDifferent parameters are used to control the model’s tolerance for misclassification.\nHyperparameters like C and Gamma (or Sigma) control SVM wiggle.\nHard versus soft margins."
  },
  {
    "objectID": "Week 6 - Classification I.html#application",
    "href": "Week 6 - Classification I.html#application",
    "title": "Week 6 - Classification I",
    "section": "6.2 Application",
    "text": "6.2 Application\nRemote sensing image scene recognition is the process of accurately labeling specific remote sensing images according to established semantic categories. Research on remote sensing image scene categorization has made significant progress due to the needs driven by application areas such as urban planning, disaster prevention, environmental monitoring and vegetation assessment.\n\n\n\n\n\nRemote sensing image scene classification（Source, Cheng, et al., 2020）\n\nUrban planning\n\nLongbotham, et al. (2012) optimized the performance of a classifier using multi-angle and multi-spectral imagery provided by the WorldView-2 satellite in combination with elevation data as well as textural, morphological and spectral features of the images. This approach effectively improves the accuracy of identifying and classifying urban elements, especially those urban features that are difficult to distinguish in a single image, thus highlighting the critical role of image classification in resolving complex urban environments.\n\nDisaster monitoring\n\nIn a study by Cheng, et al. (2013), a new technique for automatic landslide identification using remotely sensed images was proposed. The technique incorporates a bag of visual words (BoVW) model, unsupervised probabilistic latent semantic analysis (pLSA), and a k-nearest neighbor (k-NN) classifier to identify landslide areas. In the study, the remotely sensed images were divided into sub-image blocks of the same size and described using the BoVW model, landslide-related features were extracted by the pLSA model, and finally the k-NNN classifier was applied to classify the sub-image blocks into landslide and non-landslide. In the remote sensing image test in Yili area, the method proved to be effective in detecting landslides even in the absence of 3D terrain data, which is significant for landslide hazard mapping and risk assessment.\n\nEnvironmental monitoring\n\nZhang and Hunag (2018) This study uses high-resolution multi-temporal data to delve into the changing landscape of urban impervious surfaces. Taking the rapidly urbanizing city of Shenzhen as an example and combining QuickBird, WorldView-2 and WorldView-3 satellite images, the researchers developed a method that fuses multiple features to successfully identify and track changes in impervious surfaces. The study emphasizes the importance of high-resolution imagery in monitoring changes in impervious surfaces during urban development.\n\nVegetation analysis\n\nLi and Shao (2018) study proposed a high-precision classification technique for urban vegetation that effectively integrates spectral, spatial, and geometric information by utilizing 1-meter-resolution four-band digital aerial photography data and combining hierarchical classification with four segmentation techniques. This method does not require other auxiliary data and proves its usefulness in effectively recognizing various types of urban vegetation.\n\n\n\n\n\nClassification result at different levels and the categories shown in different colours (Source: Li and Shao, 2018).\nFuture Trends\nCheng et al. (2020) investigated several potential trends in remote sensing image scene recognition that may be developed in the future, including improving discriminative feature learning, multi-scale feature learning, multi-label classification, constructing large-scale datasets, unsupervised learning, compact and efficient model design, and exploration of cross-domain classification techniques."
  },
  {
    "objectID": "Week 6 - Classification I.html#refelection",
    "href": "Week 6 - Classification I.html#refelection",
    "title": "Week 6 - Classification I",
    "section": "6.3 Refelection",
    "text": "6.3 Refelection\nSo far, this class should be the most interesting content for me, classifying remote sensing images through the use of machine learning. Last semester, the last assignment for casa001 was to analyze the advantages and disadvantages of self-supervised learning for image classification, which I thought was just pretty interesting at the time, but the machine learning related algorithms involved were still complex enough to make me realize that scene classification can be challenging. I have studied binary trees in my undergraduate studies, so the knowledge related to classification and regression trees and random forests is very similar and easy to understand. Recently, I have been reading a lot of literature to complete my thesis proposal, and many papers use machine learning models to predict. I hope I can also learn to apply machine learning methods such as random forests in my thesis."
  },
  {
    "objectID": "Week 6 - Classification I.html#references",
    "href": "Week 6 - Classification I.html#references",
    "title": "Week 6 - Classification I",
    "section": "6.4 References",
    "text": "6.4 References\nCART (Classification And Regression Tree) in Machine Learning (2023) Available at: https://www.geeksforgeeks.org/cart-classification-and-regression-tree-in-machine-learning/ (Accessed: 7 March 2024).\nCheng, G., Guo, L., Zhao, T., Han, J., Li, H. and Fang, J. (2013) ‘Automatic landslide detection from remote-sensing imagery using a scene classification method based on BOVW and PLSA’, Internal Journal of Remote Sensing, 34(1/2), pp. 45-59.\nCheng, G., Xie, X. X., Han, J. W., Guo, L. and Xia, G. S. (2020) ‘Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities’, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, pp. 3735-3756.\nLi, X. and Shao, G. F. (2013) ‘Object-based urban vegetation mapping with high-resolution aerial photography as a single data source’, Internal Journal of Remote Sensing, 34(3), pp. 771-789.\nLongbotham, N., Chaapel, C., Bleiler, L., Padwick, C., Emery, W. J. and Pacifici, F. (2012) ‘Very high resolution multiangle urban classification analysis’, IEEE Transactions on Geoscience and Remote Sensing, 50(4), pp. 1155-1170.\nTran, Q., Nguyen, H. and Bui, X. (2023) ‘Novel Soft Computing Model for Predicting Blast-Induced Ground Vibration in Open-Pit Mines Based on the Bagging and Sibling of Extra Trees Models’, Computer Modeling in Engineering & Sciences, 134(3), pp. 2227-2246.\nZhang, T. and Huang, X. (2018) ‘Monitoring of urban impervious surfaces using time series of high-resolution remote sensing images in rapidly urbanized areas: A case study of Shenzhen’, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 11(8), pp. 2692-2708."
  },
  {
    "objectID": "Week 7 - Classification II.html#summary",
    "href": "Week 7 - Classification II.html#summary",
    "title": "Week 7 - Classification II",
    "section": "7.1 Summary",
    "text": "7.1 Summary\n\nObject Based Image Analysis (OBIA)\n\nDetermines the number of superpixels (k) by considering similarities or differences between cells (pixels)\nThe SLIC algorithm, Simple Linear Iterative Clustering, is the most common method for generating superpixels.\nPixels in an image are iteratively assigned to the center of the nearest superpixel while ensuring that the generated superpixels are color and spatially consistent.\nObjects are categorized using the extracted features and the calculated mean.\n\n\n\nSub Pixel Analysis\n\nSub pixel classification = Linear spectral unmixing = Spectral Mixture Analysis (SMA) - determines the proportion or abundance of landcover per pixel\n\n\n\nAccuracy assessment\n\n(Source: Cross Validated, 2024)\n\nErrors of omission = False Negative (FN) = Type II Error\nErrors of commission = False Positive (FP) = Type I Error\n\n\n\nKappa\n\nIndicates the accuracy of the image compared to chance results\nRanges from 0 to 1: The closer the Kappa is to 1, the higher the agreement between evaluators.\n\n\n\nData is not balanced\n\nVery high producer accuracy (recall) and very high user accuracy (precision) cannot exist at the same time.\nModel with high recall (Producer accuracy) = true positives but some false positives (predicted urban but land cover that isn’t urban)\nModel with high precision (User’s accuracy) = actual urban but predicted other landcover\nUser’s Accuracy (Precision) = \\(\\frac{TP}{TP + FP}\\)\nProducer’s Accuracy (Recall) = \\(\\frac{TP}{TP + FN}\\)\nThe F1-Score (or F Measure) combines both recall (Producer accuracy) and Precision (User accuracy)\nF1 = \\(\\frac{TP}{TP + \\frac{1}{2}(FP + FN)}\\)\n\nValue from 0 to 1, where 1 is better performance\nNo True Negatives (TN) in the equation\nPrecision (producer): how many positive points are correct\nRecall (user): how precise the model is at positive predictions\n\n\nHow do we get test data for the accuracy assessment?\n\nSame process for all:\nClass definition\nPre-processing\nTraining\nPixel assignment\nAccuracy assessment\nConsider a sampling strategy\nRandom sampling\nSystematic sampling\nStratified sampling\nJensen\n\nWhat data are used to assess the accuracy of classification models\n\nTrain and test split\nBest approach - cross validation: take the mean accuracy\nLeave one out cross validation: Each sample in turn is left as a test set while all the remaining samples constitute the training set. This process is repeated until each sample has been validated once as a test set.\n\n\n\nSpatial cross validation\n\nAvoided training data and test data close to each other (There can be spatial autocorrelation)\nIf a classificaiton model doesn’t consider spatial autocorrelation: The model will have better accuracy that it actually does."
  },
  {
    "objectID": "Week 7 - Classification II.html#application",
    "href": "Week 7 - Classification II.html#application",
    "title": "Week 7 - Classification II",
    "section": "7.2 Application",
    "text": "7.2 Application\nThis week explores OBIA related applications and future directions. When I was searching for OBIA related literature, I found that OBIA has a wide range of applications involving environmental monitoring, land cover, etc. I was most interested in the article I read about snowpack.\nThompson and Lees (2014) investigated an object-based image analysis technique for extracting snowpack features from remotely sensed data. The technique mostly accurately matched field observations, although the correlation was weak for short-term snowpack. However, the method is not seasonally restricted and effectively identified transient snowpack events at the beginning and end of the season in 2009. This study contributes to a better understanding and prediction of how climate change affects snowpack patterns.\n\n\n\n\n\n(Thompson and Lees, 2014)\nNot only that, OBIA is also linked to machine learning.Liu, et al. (2019) study proposed a new method combining Object-Oriented Image Analysis (OBIA) and Convolutional Neural Networks (CNNs) for land use and land cover (LULC) mapping using Sentinel optical and synthetic aperture radar (SAR) data. The method solves the problem of mismatch between CNN processing units and OBIA processing units by labeling each image object after the CNN generates the classification map. The method outperforms the traditional OBIA support vector machine (SVM) and random forest (RF) algorithms in terms of classification accuracy. In addition, it can effectively extract spatial features and maintain target boundaries, which significantly improves the classification accuracy of urban ground targets. This study provides future assistance when dealing with complex urban landscapes and contributes to understanding and managing land cover changes during urbanization.\nEz-zahounai et al. (2023) stated that the future research trend in OBIA is to combine it with deep learning in order to improve the accuracy, efficiency and interpretability of image analysis while reducing the reliance on large amounts of labeled data. However, this will be a challenge due to the lack of interpretability of deep learning."
  },
  {
    "objectID": "Week 7 - Classification II.html#reflection",
    "href": "Week 7 - Classification II.html#reflection",
    "title": "Week 7 - Classification II",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nI feel that this class is quite interesting, object-based image analysis, I reviewed a lot of literature, I know that it is used in many aspects, and it is combined with machine learning is the trend of the future development, but sub-pixel analysis, I due to the limited time this week, I did not study the relevant knowledge and application of the knowledge, and I know that he has contributed to the land cover, I am curious to see what methods it can be combined with and in what areas it can be used to make great advances. In addition, this class also involves spatial interaction, related to spatial autocorrelation, which is very familiar to me, and this knowledge I have learned in the first semester of spatial autocorrelation and spatial regression analysis. So it is tempting to apply this spatial cross-validation to my thesis: spatial differentiation of house prices and the factors affecting it, which can better reflect the performance of the model in spatial data."
  },
  {
    "objectID": "Week 7 - Classification II.html#references",
    "href": "Week 7 - Classification II.html#references",
    "title": "Week 7 - Classification II",
    "section": "7.4 References",
    "text": "7.4 References\nCross Validated (2024) Available at: https://stats.stackexchange.com/questions/122225/what-is-the-best-way-to-remember-the-difference-between-sensitivity-specificity (Accessed: 8 March 2024).\nEz-zahouani, B., Teodoro, A., Kharki, O. E., Liu, J. H., Kotaridis, I., Yuan, X. H. and Ma, L. (2023) ‘Remote sensing imagery segmentation in object-based analysis: A review of methods, optimization, and quality evaluation over the past 20 years’, Remote Sensing Applications: Society and Environment, 32, 101031.\nLiu, S. J., Qi, Z. X., Li. X. and Yeh, A. G. (2019) ‘Integration of Convolutional Neural Networks and Object-Based Post-Classification Refinement for Land Use and Land Cover Mapping with Optical and SAR Data’, Urban Remote Sensing, 11(6), 690.\nThompson, J. A. and Lees, B.G. (2014) ‘Applying object-based segmentation in the temporal domain to characterise snow seasonality’, ISPRS Journal of Photogrammetry and Remote Sensing, 97, pp. 98-110."
  },
  {
    "objectID": "Week 8 - SAR.html#summary",
    "href": "Week 8 - SAR.html#summary",
    "title": "Week 8 - SAR",
    "section": "8.1 Summary",
    "text": "8.1 Summary\n\nSynthetic Aperture Radar(SAR)\n\nActive sensors\nHave surface texture data\nSee through weather and clouds\nDifferent wavelengths - different applications\n\n\n\n\n\n\n(Source: Spaceborne Synthetic Aperture Radar Performance Prediction, 2023)\nHow SAR works\n\nEmits electromagnetic signals and records the reflected signals (backscatter).\nTransforms the phase of the signal as the radar moves to achieve high-resolution imaging.\nMultiple images are synthesized into a “synthetic” aperture to improve image quality.\n\nSAR polarization\n\nRough scattering (e.g. bare earth) = most sensitive to VV\nVolume scattering (e.g. leaves) = cross, VH or HV\nDouble bounce (e.g. trees / buildings) = most sensitive to HH.\nNotes: The first “H” or “V” of the polarization letter refers to the polarization of the transmitted signal, while the second refers to the polarization of the received signal.\n\nA SAR signal has both amplitude (backscatter) and phase data.\nBackscatter (amplitude)\n\nPolarization\n\nVV = surface roughness\nVH = volume of surface\n\nPermativity (dielectric constant) - how reflective is the property (reflective back to the sensor).\nThe return value, remember the band (wavelength)\nPhase: Location in the cycle after return\n\nInSAR: Reveals surface topography by comparing phase differences in two or more SAR images.\nDInSAR: Identifies surface deformation between observations by comparing phase differences in two SAR images in combination with external topographic data (e.g., Digital Elevation Model, DEM).\nIn GEE, only the amplitude (backscatter).\nSAR data values\n\npower scale (RAW data) = analysis\namplitude scale = visualisation\ndB scale (in GEE) = dark pixel differences\n\nIdentifying change\n\nSubtract images\n(Original) ratio images\nImproved ratio (IR)\nMean ratio images\nLog ratio images\nImproved ratio log ratio image\n\nThe variance over time through:\n\nT-test\nStandard deviation\n\nFusing SAR data to optical data\n\nPrincipal component analysis\nObject based image analysis\nIntensity fusion"
  },
  {
    "objectID": "Week 8 - SAR.html#application",
    "href": "Week 8 - SAR.html#application",
    "title": "Week 8 - SAR",
    "section": "8.2 Application",
    "text": "8.2 Application\nThis section looks at related applications of SAR.\nBayındır et al. (2017) explored the use of synthetic aperture radar (SAR) images and change detection algorithms to monitor oil spills. The study compares two algorithms: the correlation coefficient change statistic and the intensity ratio change statistic, and introduces a new method to improve detection accuracy by accumulating changes in time series images. In a case study of the oil spill in the Gulf of Mexico, the intensity-ratio change statistic shows better oil spill detection results, and the new method effectively reduces the false alarm rate and improves the reliability of oil spill detection.\nHowever, synthetic aperture radar (SAR) image ship detection systems are challenged by adversarial attacks that mislead the model by adding subtle disturbances to the input samples, leading to false outputs with high confidence. In order to improve the stability and security of the system, attack algorithms need to be investigated to enhance the model’s generalization and defense capabilities.Gao et al. (2023) proposed an attack algorithm based on Gaussian noise attributes, which not only significantly affects the accuracy of the SAR ship detection model, but also improves the model’s resistance to attacks and generalization during defense training. By filtering the attack data, the defense performance of the model can be effectively improved."
  },
  {
    "objectID": "Week 8 - SAR.html#reflection",
    "href": "Week 8 - SAR.html#reflection",
    "title": "Week 8 - SAR",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nI found it very interesting to learn about the principles of SAR in this class, and I was shocked to know that remote sensing technology nowadays can actually monitor the small deformation of the earth’s surface through SAR images acquired at different times. Since SAR involves complex principles and algorithms, systematic learning is still difficult and challenging for me to establish a complete knowledge system. If I overcome these challenges, I can not only improve my professional skills, but also enhance my problem-solving ability and self-confidence. My friend recommended me a book recommended “Synthetic Aperture Radar Imaging Algorithms and Implementations” difficult, but able to in-depth understanding and some more detailed theoretical support. Before I read this book, combing and summarizing the basics such as linear FM signals, high azimuthal resolution principles, etc., in order to build a framework for SAR technology, laying the cornerstone for my understanding of more advanced concepts. Moreover, I was amazed to see a TV news story earlier where SAR technology revealed ancient ruins hidden beneath the surface. As the technology advances, I expect that SAR will lead to more innovative and interesting applications in the future."
  },
  {
    "objectID": "Week 8 - SAR.html#references",
    "href": "Week 8 - SAR.html#references",
    "title": "Week 8 - SAR",
    "section": "8.3 References",
    "text": "8.3 References\nBayındır, C., Frost, J.D. and Barnes, C.F. (2017) ‘Assessment and enhancement of sar noncoherent change detection of sea-surface oil spills’, IEEE Journal of Oceanic Engnieering, 43, pp. 211–220.\nGao, W., Liu Y.Q., Zeng, Y., Liu Quangyang and Li, Q. (2023) ‘SAR Image Ship Target Detection Adversarial Attack and Defence Generalization Research’, Image Denoising and Image Super-resolution for Sensing Application, 23(4), pp. 2266.\nSpaceborne Synthetic Aperture Radar Performance Prediction (2023)Available at: https://uk.mathworks.com/help/radar/ug/spaceborne-synthetic-aperture-radar-performance-prediction.html (Accessed: 12 March 2024)."
  }
]